{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e965f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide Ç\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bf54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets/mics_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e83241",
   "metadata": {},
   "source": [
    "#  Class for RIR measurement databases\n",
    "> Class to get the acoustic time-series and other meta-data of RIR acoustic measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "# For testing and adding methods to a class as patches\n",
    "from fastcore.all import patch, test_eq\n",
    "# For abstract base classes\n",
    "from abc import ABC, abstractmethod\n",
    "# For type hinting\n",
    "from typing import Optional, List, Union, Tuple, ClassVar\n",
    "from urllib.error import URLError\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc1062",
   "metadata": {},
   "source": [
    "## Helper funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b052fe2",
   "metadata": {},
   "source": [
    "We will define many class properties with ``@property`` and to make sure all the attributes are initialized we define the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71adf03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramonpr/miniforge3/envs/DSenv/lib/python3.13/site-packages/nbdev/doclinks.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources,importlib\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e52953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "#| hide \n",
    "\n",
    "# def checked_property(attr_name: str, attr_type: type = object):\n",
    "#     \"\"\" \n",
    "#     Ensures that the attribute is initialized before accessing it. \n",
    "#     \"\"\"\n",
    "#     def getter(self):\n",
    "#         value = getattr(self, attr_name)\n",
    "#         if value is None:\n",
    "#             raise ValueError(f\"Attribute '{attr_name}' is not initialized.\")\n",
    "#         return value\n",
    "#     return property(getter)\n",
    "\n",
    "\n",
    "def checked_property(attr_name: str, attr_type: type = object, doc: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Ensures that the attribute is initialized before accessing it.\n",
    "    \"\"\"\n",
    "    def getter(self):\n",
    "        value = getattr(self, attr_name)\n",
    "        if value is None:\n",
    "            raise ValueError(f\"Attribute '{attr_name}' is not initialized.\")\n",
    "        return value\n",
    "    \n",
    "    prop = property(getter)\n",
    "    if doc:\n",
    "        prop.__doc__ = doc\n",
    "    return prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64ed04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Ramon-PR/DataScience_exploration/blob/main/DataScience_exploration/datasets/mics_databases.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### checked_property\n",
       "\n",
       ">      checked_property (attr_name:str, attr_type:type=<class 'object'>,\n",
       ">                        doc:Optional[str]=None)\n",
       "\n",
       "*Ensures that the attribute is initialized before accessing it.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Ramon-PR/DataScience_exploration/blob/main/DataScience_exploration/datasets/mics_databases.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### checked_property\n",
       "\n",
       ">      checked_property (attr_name:str, attr_type:type=<class 'object'>,\n",
       ">                        doc:Optional[str]=None)\n",
       "\n",
       "*Ensures that the attribute is initialized before accessing it.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(checked_property)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a153409",
   "metadata": {},
   "source": [
    "Example of use:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a764a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mics(ABC):  \n",
    "    _fs: Optional[int] = None  \n",
    "    fs = checked_property('_fs', float)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e953be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Caught ValueError: Attribute '_fs' is not initialized.\n"
     ]
    }
   ],
   "source": [
    "mic = Mics()\n",
    "print(mic._fs)  # ``_fs`` is None,\n",
    "try:\n",
    "    print(mic.fs)  # ❌ But the property ``fs`` requires _fs to be initialized to a float value\n",
    "except ValueError as e:\n",
    "    print(f\"Caught ValueError: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df2082",
   "metadata": {},
   "source": [
    "And another function that is useful, to avoid loading a full numpy file and read the header information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6285af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "#| hide \n",
    "def read_npy_header(file_path:str) -> Tuple[Tuple[int, ...], bool, np.dtype]:\n",
    "    \"\"\" Useful to read the shape of the array in a numpy file without loading the entire file into memory.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        version = np.lib.format.read_magic(f)\n",
    "        if version == (1, 0):\n",
    "            shape, fortran_order, dtype = np.lib.format.read_array_header_1_0(f)\n",
    "        elif version == (2, 0):\n",
    "            shape, fortran_order, dtype = np.lib.format.read_array_header_2_0(f)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported .npy file version: {version}\")\n",
    "    return shape, fortran_order, dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4d373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Ramon-PR/DataScience_exploration/blob/main/DataScience_exploration/datasets/mics_databases.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### read_npy_header\n",
       "\n",
       ">      read_npy_header (file_path:str)\n",
       "\n",
       "*Useful to read the shape of the array in a numpy file without loading the entire file into memory.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Ramon-PR/DataScience_exploration/blob/main/DataScience_exploration/datasets/mics_databases.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### read_npy_header\n",
       "\n",
       ">      read_npy_header (file_path:str)\n",
       "\n",
       "*Useful to read the shape of the array in a numpy file without loading the entire file into memory.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(read_npy_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ba1c8",
   "metadata": {},
   "source": [
    "## Database for microphones\n",
    "> The base class to handle RIR measurements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb74e",
   "metadata": {},
   "source": [
    "This class defines common properties and methods for the different RIR databases that will inherit from it.\n",
    "The class DB_microphones will be an abstract class (from abc import ABC, abstractmethod) \n",
    "\n",
    "+ **ABC**: base clase to declare an **A**bstract **B**ase **C**lass  \n",
    "+ **abstractmethod**: it is a decorator to indicate which methods have to be implemented by the subclasses  \n",
    "\n",
    "This is useful since this base class can not be implemented and will force the subclasses to implement certain methods `abstractmethod`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ff254",
   "metadata": {},
   "source": [
    "\n",
    "Inspired by MNIST dataset, we will download the data in a folder structure like `./root/class_name/raw`.\n",
    "\n",
    "+ **root**: is a parameter passed to the class\n",
    "+ **class_name**: is the name of the class used to download the database  \n",
    "+ **raw**: is the subfolder where the raw data is downloaded  \n",
    "\n",
    "and we will include a `mirror` list with the urls where we can find the data to download, and a list `resources` with the name of the file to download and it's md5 checksum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383cb823",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44de29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DB_microphones(ABC):\n",
    "    \"\"\"\n",
    "        Base class for microphone databases.\n",
    "        I define the @property methods here, so I don't have to redefine them in the subclasses.\n",
    "    \"\"\"\n",
    "\n",
    "    # ClassVar tells Pylance that these are Class variables, not instance variables.\n",
    "    # and initializes them to empty lists (although __init_subclass__ will ensure they are defined in subclasses)\n",
    "    mirrors: ClassVar[list[str]] = [] # List of urls to download the data from.\n",
    "    resources: ClassVar[list[tuple[str, str]]] = [] # List with tuples (filename, md5) for the files to download.\n",
    "\n",
    "    # This method is called when a subclass is defined. And I use it to ensure that the subclass has the required class attributes.\n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        if not hasattr(cls, 'resources'):\n",
    "            raise NotImplementedError(f\"{cls.__name__} must define class attribute 'resources'\")\n",
    "        if not hasattr(cls, 'mirrors'):\n",
    "            raise NotImplementedError(f\"{cls.__name__} must define class attribute 'mirrors'\")\n",
    "\n",
    "\n",
    "    _fs: Optional[float] # Using Optional to indicate that these attributes can be None until initialized\n",
    "    _nmics: Optional[int]\n",
    "    _nt: Optional[int]\n",
    "    _n_sources: Optional[int]\n",
    "    _source_id: Optional[int]\n",
    "    _signal_size: Optional[int]\n",
    "    _signal_start: Optional[int]\n",
    "    \n",
    "    def __init__(self, \n",
    "                 root: str = \"./data\", # Path to the root directory of the database, where the data will be dowloaded \n",
    "                 dataname: str = \"RIR\", # String matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded). \n",
    "                 signal_start: int = 0, # Start index of the signal in the data\n",
    "                 signal_size: Optional[int] = None, # int or None. Size of the signal to be extracted from the data, if None, the whole signal will be loaded.\n",
    "                 ):\n",
    "        \n",
    "        self.root = root\n",
    "        self._signal_start = signal_start\n",
    "        self._signal_size = signal_size\n",
    "        self._nt = None\n",
    "\n",
    "        self._fs = None\n",
    "        self._nmics = None\n",
    "        self._n_sources = None\n",
    "        self._source_id = None\n",
    "\n",
    "        self._resources_to_download =  self._matching_resources(pattern=dataname)\n",
    "        self._resource_to_load = self._resources_to_download[0][0] if self._resources_to_download else None\n",
    "        assert self._resource_to_load is not None, f\"No resources found matching '{dataname}'.\"\n",
    "\n",
    "        self._resource_datapath = os.path.join(self.raw_folder, self._resource_to_load)\n",
    "        \n",
    "        # Create the root directory if it does not exist\n",
    "        Path(self.root).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_data(self, filepath: str):\n",
    "        \"\"\" Load the data from the given filepath.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Validated properties with documentation\n",
    "    fs = checked_property(\"_fs\", float, \"Sampling frequency in Hz\")\n",
    "    n_mics = checked_property(\"_nmics\", int, \"Number of microphones\")\n",
    "    nt = checked_property(\"_nt\", int, \"Number of total time samples in the database\")\n",
    "    n_sources = checked_property(\"_n_sources\", int, \"Number of sound sources\")\n",
    "    source_id = checked_property(\"_source_id\", int, \"ID of the current source\")\n",
    "    signal_size = checked_property(\"_signal_size\", int, \"Size of the signal to extract\")\n",
    "    signal_start = checked_property(\"_signal_start\", int, \"Start index of the signal\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> str:\n",
    "        \"\"\" Returns the path to the raw data folder. ./data/class_name/raw \"\"\"\n",
    "        return os.path.join(self.root, self.__class__.__name__, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> float:\n",
    "        return 1.0 / self.fs  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_mic(self, imic: int, start: int, size: int) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_pos(self, imic: int) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def get_time(self, start: int, size: int) -> np.ndarray:\n",
    "        return (start + np.arange(size)) * self.dt\n",
    "    \n",
    "    def _matching_resources(self,\n",
    "                         pattern: str, # pattern to look for in resource names\n",
    "                         ) -> list:\n",
    "        \"\"\" match if the pattern is found in any of the resources \"\"\" \n",
    "\n",
    "        if not hasattr(self, 'resources'):\n",
    "            print(\"No resources found.\")\n",
    "            return []\n",
    "\n",
    "        # Assuming self.resources is a list of tuples (resource_name, resource_data)\n",
    "\n",
    "        # where resource_name is a string and resource_data can be any type\n",
    "        matches = [(res, md5) for res, md5 in self.resources if pattern.lower() in res.lower()]\n",
    "        return(matches)\n",
    "\n",
    "    \n",
    "    def _download_resource(self, \n",
    "                           resource_name: str, # name of the resource to download\n",
    "                            ) -> None:\n",
    "        \n",
    "        \"\"\" download a resource by its name \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'resources'):\n",
    "            print(\"No resources found.\")\n",
    "            return\n",
    "\n",
    "        # Check the matching resources\n",
    "        down_resources = self._matching_resources(pattern = resource_name)\n",
    "        if not down_resources:\n",
    "            print(f\"No resources found matching '{resource_name}'.\")\n",
    "            return\n",
    "\n",
    "        for file, md5 in down_resources:\n",
    "            errors = []\n",
    "            for mirror in self.mirrors:\n",
    "                url = os.path.join(mirror, file)\n",
    "                try:\n",
    "                    if not os.path.isfile(os.path.join(self.raw_folder, file)):\n",
    "                        print(f\"Downloading {file} from {mirror}\")\n",
    "                        download_url(url=url, root=self.raw_folder, filename=file, md5=md5)\n",
    "\n",
    "                except URLError as e:\n",
    "                    errors.append(e)\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                s = f\"Error downloading {file}:\\n\"\n",
    "                for mirror, err in zip(self.mirrors, errors):\n",
    "                    s += f\"Tried {mirror}, got:\\n{str(err)}\\n\"\n",
    "                raise RuntimeError(s)\n",
    "            \n",
    "    def _prepare_download_and_unpack(self, \n",
    "                          dataname: str, # Sting \n",
    "                          unpack: bool = True\n",
    "                          ) -> str:\n",
    "        \"\"\"\n",
    "        Common workflow: match resource, download if needed, unpack, and load.\n",
    "        Child classes should call this and implement their own load_data.\n",
    "        \"\"\"\n",
    "        matched_res = self._matching_resources(dataname)\n",
    "        if not matched_res:\n",
    "            raise ValueError(f\"No resources found matching '{dataname}'.\")\n",
    "\n",
    "        print(\"Matched resources to download:\")\n",
    "        for res, _ in matched_res:\n",
    "            print(f\"- {res}\")\n",
    "\n",
    "        # Download the resource if it does not exist in the raw folder \n",
    "        self._download_resource(resource_name=dataname)\n",
    "\n",
    "        # Unpack the resource if needed\n",
    "        if unpack:\n",
    "            self.data_folder = self._unpack_resource() # Unpacked folder with the data\n",
    "            return self.data_folder\n",
    "        else:\n",
    "            self.data_path = self._resource_datapath # If it does not need unpacking, just return the path to the resource file\n",
    "            return self._resource_datapath\n",
    "\n",
    "\n",
    "    def _unpack_resource(self) -> str:\n",
    "        \"\"\" Unpack the resource if it is compressed. \"\"\"\n",
    "\n",
    "        # path del resource sin unpack\n",
    "        assert os.path.exists(self._resource_datapath), f\"Resource {self._resource_datapath} does not exist. Please download it first.\" \n",
    "\n",
    "        # Check if the unpacked folder is already there\n",
    "        unpacked_folder = os.path.splitext(self._resource_datapath)[0]\n",
    "\n",
    "        if os.path.exists(unpacked_folder):\n",
    "            print(f\"Unpacked folder {unpacked_folder} already exists. Skipping unpacking.\")\n",
    "            return unpacked_folder\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                extract_archive(from_path=self._resource_datapath)\n",
    "                print(f\"Unpacked {self._resource_datapath} to {unpacked_folder}\")\n",
    "                return unpacked_folder\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error unpacking {self._resource_datapath}: {e}\")\n",
    "                return unpacked_folder  # Return the folder even if there was an error\n",
    "                \n",
    "                \n",
    "    @classmethod\n",
    "    def print_resources(cls):\n",
    "        print(f\"Resources for class {cls.__name__}:\")\n",
    "        for name, md5 in cls.resources:\n",
    "            print(f\"- {name} \")\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Database: {self.__class__.__name__}\\n\"\n",
    "            f\"Download: {[resname for resname, _ in self._resources_to_download] }\\n\"\n",
    "            f\"Load room: {self._resource_to_load}\\n\"\n",
    "            f\"Path to raw resource: {self._resource_datapath}\\n\"\n",
    "            f\"Path to unpacked data folder: {self.data_folder}\\n\"\n",
    "            f\"Sampling frequency: {self.fs} Hz\\n\"\n",
    "            f\"Number of microphones: {self.n_mics}\\n\"\n",
    "            f\"Number of total time samples: {self.nt}\\n\"\n",
    "            f\"Number of time samples selected: {self.signal_size}\\n\"\n",
    "            f\"Number of sources: {self.n_sources}\\n\"\n",
    "            f\"Signal start: {self.signal_start}\\n\"\n",
    "            f\"Signal size: {self.signal_size}\\n\"\n",
    "            f\"Source ID: {self.source_id}\"\n",
    "        )\n",
    "    \n",
    "    def _check_bounds_in_sample_size(self, number_of_time_samples: int) -> None:\n",
    "        \"\"\" Check if the start and size are within the bounds of the signal size. \"\"\"\n",
    "\n",
    "        T = number_of_time_samples\n",
    "        assert self._signal_start is not None\n",
    "        start_sample = self._signal_start\n",
    "        if self._signal_size is None:\n",
    "            self._signal_size = T - start_sample\n",
    "\n",
    "        assert self._signal_size is not None\n",
    "        last_sample = self._signal_start + self._signal_size\n",
    "\n",
    "        assert (start_sample >= 0 and start_sample < T), f\"The start_signal should be in [0, {T-1}].\"\n",
    "        assert (last_sample > 0 and last_sample <= T), f\"The size_signal should be in [1, {T-start_sample}].\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1534bc",
   "metadata": {},
   "source": [
    "### Zea database\n",
    "> Database from [Elias Zea](https://www.sciencedirect.com/science/article/abs/pii/S0022460X19304316) . It will inherit from DB_micorphones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68c393",
   "metadata": {},
   "source": [
    "This is one of the RIR databases. It will have to implement it's own attributes:  \n",
    "    + `mirrors`  \n",
    "    + `resources`  \n",
    "    + `microphone spacing`  \n",
    "\n",
    "And the methods:  \n",
    "    + To check what resource to load  \n",
    "    + To download the resources  \n",
    "    + To unpack the downloaded resources  \n",
    "    + To load the selected resource (database/dataname)  \n",
    "    + To get the different attributes in the database: `dx`, `dt`, `fs`, `num_mics`, `num_sources`  \n",
    "    + And also the data related with the microphone recordings: `imic`, `position`, `time_samples`, `signal`  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ZeaRIR(DB_microphones):\n",
    "    \"\"\" ZeaRIR database. \"\"\"\n",
    "\n",
    "    mirrors = [\n",
    "            \"https://raw.githubusercontent.com/eliaszea/RIRIS/main/dependencies/measurementData/\"\n",
    "        ]\n",
    "\n",
    "    resources = [\n",
    "            (\"BalderRIR.mat\", \"bc904010041dc18e54a1a61b23ee3f99\"),\n",
    "            (\"FrejaRIR.mat\", \"1dedf2ab190ad48fbfa9403409418a1d\"),\n",
    "            (\"MuninRIR.mat\", \"5c90de0cbbc61128de332fffc64261c9\"),\n",
    "        ]\n",
    "    \n",
    "    _dx = 3e-2  # Distance between microphones in meters, as per the database documentation.\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str = \"./data\", # Path to the root directory of the database, where the data will be dowloaded\n",
    "                 dataname: str = \"Balder\", # String matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded). \n",
    "                 signal_start: int = 0, # Start index of the signal to load.\n",
    "                 signal_size: Optional[int] = None, # # int or None. Size of the signal to be extracted from the data, if None, the whole signal will be loaded.\n",
    "                 ):\n",
    "        super().__init__(root, dataname, signal_start, signal_size)\n",
    "\n",
    "        # Prepare the download and unpack the resource\n",
    "        filepath = self._prepare_download_and_unpack(dataname, unpack=False)\n",
    "        \n",
    "        self.data_folder = self.raw_folder\n",
    "\n",
    "\n",
    "        # The resource *.mat is not unpacked, so we can load it directly.\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        self.load_data(filepath)\n",
    "\n",
    "\n",
    "    def load_data(self, filepath: str):\n",
    "        \"\"\" Loads all the Matlab data from the given filepath.\"\"\"\n",
    "        print(f\"Loading the resource {filepath} ...\")\n",
    "        _rawdata = loadmat(filepath, simplify_cells=True)\n",
    "        self._fs = _rawdata['out']['fs']\n",
    "\n",
    "        T = _rawdata['out']['T']\n",
    "        M = _rawdata['out']['M']\n",
    "\n",
    "        # Check if the signal_start and signal_size are within the bounds of the signal size\n",
    "        self._check_bounds_in_sample_size(number_of_time_samples=T)\n",
    "        assert isinstance(self._signal_start, int) and isinstance(self._signal_size, int)\n",
    "        start_sample = self._signal_start\n",
    "        last_sample = self._signal_start + self._signal_size\n",
    "\n",
    "        self._RIR = _rawdata['out']['image'][start_sample:last_sample, :]  # Transpose to have (n_mics, n_sources, nt)\n",
    "\n",
    "        self._nmics = M\n",
    "        self._nt = T\n",
    "        self._n_sources = 1\n",
    "        self._source_id = 0\n",
    "\n",
    "\n",
    "    def get_mic(self, imic: int, start: int, size: int) -> np.ndarray:\n",
    "        \"\"\" Returns the signal of the microphone imic, starting at index start and with size size. \"\"\"\n",
    "        return self._RIR[start:start + size, imic]\n",
    "    \n",
    "\n",
    "    def get_pos(self, imic: int) -> np.ndarray:\n",
    "        \"\"\" Returns the position of the microphone imic in meters (x, y, z) \"\"\"\n",
    "        assert 0 <= imic < self.n_mics, f\"Microphone index {imic} out of range [0, {self.n_mics - 1}]\"\n",
    "        return  np.array([imic * self._dx, 0, 0])\n",
    "\n",
    "    def _unpack_resource(self):\n",
    "        \"\"\" .mat files does not need to be uncompressed. To avoid confusion, I return the path to the resource file directly. \"\"\"\n",
    "        return self._resource_datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ba658",
   "metadata": {},
   "source": [
    "#### Checks that Zea database works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched resources to download:\n",
      "- BalderRIR.mat\n",
      "- FrejaRIR.mat\n",
      "- MuninRIR.mat\n",
      "Loading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n"
     ]
    }
   ],
   "source": [
    "db = ZeaRIR(root=\"./data\", dataname=\"RIR\", signal_start=0, signal_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: ZeaRIR\n",
      "Download: ['BalderRIR.mat', 'FrejaRIR.mat', 'MuninRIR.mat']\n",
      "Load room: BalderRIR.mat\n",
      "Path to raw resource: ./data/ZeaRIR/raw/BalderRIR.mat\n",
      "Path to unpacked data folder: ./data/ZeaRIR/raw\n",
      "Sampling frequency: 11250 Hz\n",
      "Number of microphones: 100\n",
      "Number of total time samples: 3623\n",
      "Number of time samples selected: 128\n",
      "Number of sources: 1\n",
      "Signal start: 0\n",
      "Signal size: 128\n",
      "Source ID: 0\n"
     ]
    }
   ],
   "source": [
    "print(db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789f4b1",
   "metadata": {},
   "source": [
    "Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7211e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk of data of size (128, 100)\n",
      "Sample with get_mic: [ 0.00041836  0.0001148  -0.00129174  0.00162724]\n",
      "Sample with get_pos: [0.03 0.   0.  ]\n",
      "Sample with get_time: [0.00000000e+00 8.88888889e-05 1.77777778e-04 2.66666667e-04]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded chunk of data of size {db._RIR.shape}\")\n",
    "print(f\"Sample with get_mic: {db.get_mic(imic=0, start=0, size=4)}\")\n",
    "print(f\"Sample with get_pos: {db.get_pos(imic=1)}\")\n",
    "print(f\"Sample with get_time: {db.get_time(start=0, size=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de5721",
   "metadata": {},
   "source": [
    "This is how I have calculated the MD5 of each file in resources to add it in the class definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f82a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import calculate_md5, check_md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched resources to download:\n",
      "- BalderRIR.mat\n",
      "Loading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n",
      "File: BalderRIR.mat, MD5: bc904010041dc18e54a1a61b23ee3f99\n",
      "File: FrejaRIR.mat, MD5: 1dedf2ab190ad48fbfa9403409418a1d\n",
      "File: MuninRIR.mat, MD5: 5c90de0cbbc61128de332fffc64261c9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = ZeaRIR(root=\"./data\")\n",
    "for file, md5_class in db.resources:\n",
    "    url = os.path.join(db.mirrors[0], file)\n",
    "    download_url(url, root=db.raw_folder, filename=file)\n",
    "    md5 = calculate_md5(os.path.join(db.raw_folder, file))\n",
    "    print(f\"File: {file}, MD5: {md5}\")\n",
    "    assert check_md5(os.path.join(db.raw_folder, file), md5_class), (\n",
    "    f\"Check the MD5 of the resource '{file}' for the class '{db.__class__.__name__}' \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08caa8",
   "metadata": {},
   "source": [
    "Here we implement the option to print the resources that can be downloaded.  \n",
    "I use `@patch` from `fastcore` to add this function to the class after the class has already been defined.  \n",
    "Since we just want to print the resources (class attributes), it is a class method, so it does not need an instance of the class.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848854c5",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-note}\n",
    "Pylance linting does not like `patch` and will underline it as a possible error.  \n",
    "I have added it directly to the class (the following code is just for testing purposes).\n",
    "([This is a callout from Quarto](https://quarto.org/docs/authoring/callouts.html#callout-types))\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)  \n",
    "def print_resources(cls: DB_microphones):\n",
    "    print(f\"Resources for class {cls.__name__}:\")\n",
    "    for name, md5 in cls.resources:\n",
    "        print(f\"- {name} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe16a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources for class ZeaRIR:\n",
      "- BalderRIR.mat \n",
      "- FrejaRIR.mat \n",
      "- MuninRIR.mat \n"
     ]
    }
   ],
   "source": [
    "ZeaRIR.print_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215f48c",
   "metadata": {},
   "source": [
    "Now let's implement a method to recognize if a string pattern provided as dataname matches any resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17139a",
   "metadata": {},
   "source": [
    "> Downloading:  \n",
    "\n",
    "We give the option to give a string pattern to download several resources,  \n",
    "but each instance of the class should be used to provide measurements of only one of the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0dc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to resource: ./data/ZeaRIR/raw/BalderRIR.mat\n"
     ]
    }
   ],
   "source": [
    "res, _ =  db._matching_resources(\"balder\")[0]\n",
    "# print(res)\n",
    "pathfname = os.path.join(db.raw_folder, res)\n",
    "print(f\"Path to resource: {pathfname}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84083a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched resources to download:\n",
      "- BalderRIR.mat\n",
      "- FrejaRIR.mat\n",
      "- MuninRIR.mat\n",
      "Loading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = ZeaRIR(root=\"./data\", dataname=\"RIR\")\n",
    "db._matching_resources(\"RIR\")\n",
    "db._download_resource(\"RIR\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f92021",
   "metadata": {},
   "source": [
    "### MeshRIR database\n",
    "> Database from [Shoichi Koyama](https://arxiv.org/abs/2106.10801), National Institute of Informatics, Tokyo, Japan . It will inherit from DB_micorphones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MeshRIR(DB_microphones):\n",
    "\n",
    "    mirrors = [\n",
    "        \"https://zenodo.org/records/10852693/files/\"\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"S1-M3969_npy.zip\", \"2cb598eb44bb9905560c545db7af3432\" ),\n",
    "        (\"S32-M441_npy.zip\", \"9818fc66b36513590e7abd071243d8e9\"), \n",
    "    ]\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str = \"./data\", # Path to the root directory of the database, where the data will be dowloaded\n",
    "                 dataname: str = \"S1\", # String matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded). \n",
    "                 signal_start: int = 0, # Start index of the signal to load.\n",
    "                 signal_size: Optional[int] = None, # Size of the signal to load. If None, the whole signal will be loaded.\n",
    "                 source_id: int = 0,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__(root, dataname, signal_start, signal_size)\n",
    "\n",
    "        # Prepare the download and unpack the resource\n",
    "        # This database unpacks files in a folder with the same name as the resource without the .zip extension \n",
    "        self.data_folder = self._prepare_download_and_unpack(dataname, unpack=True)\n",
    "        assert isinstance(self.data_folder, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "\n",
    "        # Load the data from the unpacked folder (also perform checks)\n",
    "        self._load_database_info()  # Load the database information from the data.json file\n",
    "        assert (source_id >= 0) and (source_id < self.n_sources) , f\"Database has {self.n_sources} sources. Choose source_id in [0, {self.n_sources-1}]. \"\n",
    "        self._source_id = source_id\n",
    "\n",
    "        # Loads src and mic positions, NOTE: maybe also load signals? load_all_data \n",
    "        # self.load_data(filepath=self.data_folder)       \n",
    "        self.load_src_and_mics_positions()\n",
    "\n",
    "\n",
    "    def load_data(self, filepath: str):\n",
    "        filepath=self.data_folder\n",
    "        self.load_src_and_mics_positions()\n",
    "\n",
    "    def _load_database_info(self):\n",
    "        \"\"\" Load the database information from the data.json file. \"\"\"\n",
    "        json_file = os.path.join(self.data_folder, \"data.json\")\n",
    "        with open(json_file, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        self._fs = json_data['samplerate']\n",
    "        T = json_data['ir length']\n",
    "        self._n_sources = json_data['number of sources']\n",
    "        self._nmics = json_data['number of points']\n",
    "\n",
    "        # Check that the folder contains all the signals in the database\n",
    "        nfiles = len([f for f in os.listdir(self.data_folder) if f.startswith('ir_') and f.endswith('.npy')])\n",
    "        assert self._nmics == nfiles, f\"ir_xxx.npy files = {nfiles}, should be {self._nmics}\"\n",
    "\n",
    "        # Check if the signal_start and signal_size are within the bounds of the signal size\n",
    "        self._check_bounds_in_sample_size(number_of_time_samples=T)\n",
    "        assert isinstance(self._signal_start, int) and isinstance(self._signal_size, int)\n",
    "        self._start_sample = self._signal_start\n",
    "        self._last_sample = self._signal_start + self._signal_size\n",
    "        self._nt = T\n",
    "\n",
    "    def load_src_and_mics_positions(self):\n",
    "        self.load_src_positions()  # Load source positions from the file\n",
    "        self.load_mic_positions()  # Load microphone positions from the file\n",
    "\n",
    "    def load_src_positions(self):\n",
    "        # Source position in the dataset\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        pos_src_path = os.path.join(filepath, 'pos_src.npy')\n",
    "        self._source_positions = np.load(pos_src_path)\n",
    "\n",
    "    def load_mic_positions(self):\n",
    "        # Position of the microphones\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        pos_mic_path = os.path.join(filepath, 'pos_mic.npy')\n",
    "        self._pos_mics = np.load(pos_mic_path) # (nmics, 3)  each row is (x,y,z) for a mic\n",
    "\n",
    "\n",
    "    def load_all_data(self):\n",
    "        # Concatenate vectors (source, signal) -> into -> (source, imic, signal)\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        data = np.concatenate( \n",
    "            [np.load(os.path.join(filepath, f'ir_{i}.npy'))[:,None,:]  \n",
    "             for i in range(self.n_mics)], # for all mics\n",
    "             axis = 1 ) # in axis 1 (mics)  (source, mics, signal)\n",
    "        return data\n",
    "\n",
    "    def load_mic(self, imic):\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        mic_signal = np.load(os.path.join(filepath, f'ir_{imic}.npy')) # (source, signal)\n",
    "        return mic_signal[self.source_id, :]\n",
    "        \n",
    "    def get_pos(self, imic: int)-> np.ndarray:\n",
    "        \"\"\" Returns the position of the microphone imic in meters (x, y, z) \"\"\"\n",
    "        if not hasattr(self, \"_pos_mics\"):\n",
    "            self.load_mic_positions()\n",
    "        assert 0 <= imic < self.n_mics, f\"Microphone index {imic} out of range [0, {self.n_mics - 1}]\"\n",
    "        return self._pos_mics[imic,:]\n",
    "    \n",
    "    def get_time(self, start=None, size=None):\n",
    "        return super().get_time(start=self.signal_start, size=self.signal_size)\n",
    "    \n",
    "    def get_src_pos(self):\n",
    "        if not hasattr(self, \"_source_positions\"):\n",
    "            self.load_src_positions()\n",
    "        return self._source_positions[self.source_id]\n",
    "    \n",
    "    def get_mic(self, imic: int, start: int, size: int) -> np.ndarray:\n",
    "        \"\"\" Returns the signal of the microphone imic, starting at index start and with size size. \"\"\"\n",
    "        return self.load_mic(imic=imic)[start:start + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched resources to download:\n",
      "- S32-M441_npy.zip\n",
      "Unpacked folder ./data/MeshRIR/raw/S32-M441_npy already exists. Skipping unpacking.\n"
     ]
    }
   ],
   "source": [
    "db2 = MeshRIR(root=\"./data\", dataname=\"S32\", signal_start=0, signal_size=128, source_id=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: MeshRIR\n",
      "Download: ['S32-M441_npy.zip']\n",
      "Load room: S32-M441_npy.zip\n",
      "Path to raw resource: ./data/MeshRIR/raw/S32-M441_npy.zip\n",
      "Path to unpacked data folder: ./data/MeshRIR/raw/S32-M441_npy\n",
      "Sampling frequency: 48000 Hz\n",
      "Number of microphones: 441\n",
      "Number of total time samples: 32768\n",
      "Number of time samples selected: 128\n",
      "Number of sources: 32\n",
      "Signal start: 0\n",
      "Signal size: 128\n",
      "Source ID: 31\n"
     ]
    }
   ],
   "source": [
    "print(db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
