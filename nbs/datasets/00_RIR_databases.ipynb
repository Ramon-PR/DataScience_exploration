{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e965f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bf54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets/mics_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e83241",
   "metadata": {},
   "source": [
    "#  Class for RIR measurement databases\n",
    "> Class to get the acoustic time-series and other meta-data of RIR acoustic measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "# For testing and adding methods to a class as patches\n",
    "from fastcore.all import patch, test_eq\n",
    "# For abstract base classes\n",
    "from abc import ABC, abstractmethod\n",
    "# For type hinting\n",
    "from typing import Optional, List, Union, Tuple, ClassVar\n",
    "from urllib.error import URLError\n",
    "from scipy.io import loadmat\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc1062",
   "metadata": {},
   "source": [
    "## A. Helper funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b052fe2",
   "metadata": {},
   "source": [
    "We will define many class properties with ``@property`` and to make sure all the attributes are initialized before their use, we define the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71adf03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramonpr/miniforge3/envs/pl/lib/python3.13/site-packages/nbdev/doclinks.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources,importlib\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e52953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "#| hide \n",
    "\n",
    "def checked_property(attr_name: str, # string with the name of the protected attribute to access, example: '_fs'\n",
    "                     attr_type: type = object, # Type of the attribute: for _fs for example is float\n",
    "                     doc: Optional[str] = None # String containing a descrption of the class attribute \n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Ensures that the attribute is initialized before accessing it.\n",
    "    \"\"\"\n",
    "    def getter(self):\n",
    "        value = getattr(self, attr_name)\n",
    "        if value is None:\n",
    "            raise ValueError(f\"Attribute '{attr_name}' is not initialized.\")\n",
    "        return value\n",
    "    \n",
    "    prop = property(getter)\n",
    "    if doc:\n",
    "        prop.__doc__ = doc\n",
    "    return prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64ed04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Ramon-PR/DataScience_exploration/blob/main/DataScience_exploration/datasets/mics_databases.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### checked_property\n",
       "\n",
       ">      checked_property (attr_name:str, attr_type:type=<class 'object'>,\n",
       ">                        doc:Optional[str]=None)\n",
       "\n",
       "*Ensures that the attribute is initialized before accessing it.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| attr_name | str |  | string with the name of the protected attribute to access, example: '_fs' |\n",
       "| attr_type | type | object | Type of the attribute: for _fs for example is float |\n",
       "| doc | Optional | None | String containing a descrption of the class attribute |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Ramon-PR/DataScience_exploration/blob/main/DataScience_exploration/datasets/mics_databases.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### checked_property\n",
       "\n",
       ">      checked_property (attr_name:str, attr_type:type=<class 'object'>,\n",
       ">                        doc:Optional[str]=None)\n",
       "\n",
       "*Ensures that the attribute is initialized before accessing it.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| attr_name | str |  | string with the name of the protected attribute to access, example: '_fs' |\n",
       "| attr_type | type | object | Type of the attribute: for _fs for example is float |\n",
       "| doc | Optional | None | String containing a descrption of the class attribute |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(checked_property)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a153409",
   "metadata": {},
   "source": [
    "Example of use:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a764a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mics(ABC):  \n",
    "    _fs: Optional[int] = None  \n",
    "    fs = checked_property('_fs', float)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e865d",
   "metadata": {},
   "source": [
    "And if we use the property to access ``_fs`` without it being initialized, it should give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e953be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Caught ValueError: Attribute '_fs' is not initialized.\n"
     ]
    }
   ],
   "source": [
    "mic = Mics()\n",
    "print(mic._fs)  # ``_fs`` is None,\n",
    "try:\n",
    "    print(mic.fs)  # ❌ But the property ``fs`` requires _fs to be initialized to a float value\n",
    "except ValueError as e:\n",
    "    print(f\"Caught ValueError: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ba1c8",
   "metadata": {},
   "source": [
    "## B. Database for microphones\n",
    "> The base class to handle RIR measurements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb74e",
   "metadata": {},
   "source": [
    "This class defines common properties and methods for the different RIR databases that will inherit from it.\n",
    "The class DB_microphones will be an abstract class (from abc import ABC, abstractmethod) \n",
    "\n",
    "+ **ABC**: base clase to declare an **A**bstract **B**ase **C**lass  \n",
    "+ **abstractmethod**: it is a decorator to indicate which methods have to be implemented by the subclasses  \n",
    "\n",
    "This is useful since this base class can not be implemented and will force the subclasses to implement certain methods `abstractmethod`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ff254",
   "metadata": {},
   "source": [
    "\n",
    "Inspired by MNIST dataset, we will download the data in a folder structure like `./root/class_name/raw`.\n",
    "\n",
    "+ **root**: is a parameter passed to the class\n",
    "+ **class_name**: is the name of the class used to download the database  \n",
    "+ **raw**: is the subfolder where the raw data is downloaded  \n",
    "\n",
    "and we will include a `mirror` list with the urls where we can find the data to download, and a list `resources` that contains tuples with the name of the file to download and it's md5 checksum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383cb823",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44de29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DB_microphones(ABC):\n",
    "    \"\"\"\n",
    "        Base class for microphone databases.\n",
    "        Defines methods: get_mic, get_pos, get_time and class @property such as .fs, .nt, .n_mics, .n_sources, ...\n",
    "    \"\"\"\n",
    "\n",
    "    # ClassVar tells Pylance that these are Class variables, not instance variables.\n",
    "    # and initializes them to empty lists (although __init_subclass__ will ensure they are defined in subclasses)\n",
    "    mirrors: ClassVar[list[str]] = [] # List of urls to download the data from.\n",
    "    resources: ClassVar[list[tuple[str, str]]] = [] # List with tuples (filename, md5) for the files to download.\n",
    "\n",
    "    # This method is called when a subclass is defined. And I use it to ensure that the subclass has the required class attributes.\n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        if not hasattr(cls, 'resources'):\n",
    "            raise NotImplementedError(f\"{cls.__name__} must define class attribute 'resources'\")\n",
    "        if not hasattr(cls, 'mirrors'):\n",
    "            raise NotImplementedError(f\"{cls.__name__} must define class attribute 'mirrors'\")\n",
    "\n",
    "\n",
    "    _fs: Optional[float] # Using Optional to indicate that these attributes can be None until initialized\n",
    "    _nmics: Optional[int]\n",
    "    _nt: Optional[int]\n",
    "    _n_sources: Optional[int]\n",
    "    _source_id: Optional[int]\n",
    "    _signal_size: Optional[int]\n",
    "    _signal_start: Optional[int]\n",
    "    \n",
    "    def __init__(self, \n",
    "                 root: str = \"./data\", # Path to the root directory of the database, where the data will be dowloaded \n",
    "                 dataname: str = \"RIR\", # String matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded). \n",
    "                 signal_start: int = 0, # Start index of the signal in the data\n",
    "                 signal_size: Optional[int] = None, # int or None. Size of the signal to be extracted from the data, if None, the whole signal will be loaded.\n",
    "                 ):\n",
    "        \n",
    "        self.root = root\n",
    "        self._signal_start = signal_start\n",
    "        self._signal_size = signal_size\n",
    "        self._nt = None\n",
    "\n",
    "        self._fs = None\n",
    "        self._nmics = None\n",
    "        self._n_sources = None\n",
    "        self._source_id = None\n",
    "\n",
    "        self._resources_to_download =  self._matching_resources(pattern=dataname)\n",
    "        self._resource_to_load = self._resources_to_download[0][0] if self._resources_to_download else None\n",
    "        assert self._resource_to_load is not None, f\"No resources found matching '{dataname}'.\"\n",
    "\n",
    "        self._resource_datapath = os.path.join(self.raw_folder, self._resource_to_load)\n",
    "        \n",
    "        # Create the root directory if it does not exist\n",
    "        Path(self.root).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_data(self, filepath: str):\n",
    "        \"\"\" Load the data from the given filepath.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # Validated properties with documentation\n",
    "    fs = checked_property(\"_fs\", float, \"Sampling frequency in Hz\")\n",
    "    n_mics = checked_property(\"_nmics\", int, \"Number of microphones\")\n",
    "    nt = checked_property(\"_nt\", int, \"Number of total time samples in the database\")\n",
    "    n_sources = checked_property(\"_n_sources\", int, \"Number of sound sources\")\n",
    "    source_id = checked_property(\"_source_id\", int, \"ID of the current source\")\n",
    "    signal_size = checked_property(\"_signal_size\", int, \"Size of the signal to extract\")\n",
    "    signal_start = checked_property(\"_signal_start\", int, \"Start index of the signal\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> str:\n",
    "        \"\"\" Returns the path to the raw data folder. ./data/class_name/raw \"\"\"\n",
    "        return os.path.join(self.root, self.__class__.__name__, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> float:\n",
    "        return 1.0 / self.fs  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_mic(self, imic: int, start: Optional[int]=None, size: Optional[int]=None) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_pos(self, imic: int) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def get_time(self, start: Optional[int]=None, size: Optional[int]=None) -> np.ndarray:        \n",
    "        start = start if start is not None else self.signal_start\n",
    "        size = size if size is not None else self.signal_size\n",
    "        assert isinstance(start, int)\n",
    "        assert isinstance(size, int)\n",
    "        return (start + np.arange(size)) * self.dt\n",
    "    \n",
    "    def _matching_resources(self,\n",
    "                         pattern: str, # pattern to look for in resource names\n",
    "                         ) -> list:\n",
    "        \"\"\" match if the pattern is found in any of the resources \"\"\" \n",
    "\n",
    "        if not hasattr(self, 'resources'):\n",
    "            print(\"No resources found.\")\n",
    "            return []\n",
    "\n",
    "        # Assuming self.resources is a list of tuples (resource_name, resource_data)\n",
    "\n",
    "        # where resource_name is a string and resource_data can be any type\n",
    "        matches = [(res, md5) for res, md5 in self.resources if pattern.lower() in res.lower()]\n",
    "        return(matches)\n",
    "\n",
    "    \n",
    "    def _download_resource(self, \n",
    "                           resource_name: str, # name of the resource to download\n",
    "                            ) -> None:\n",
    "        \n",
    "        \"\"\" download a resource by its name \"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'resources'):\n",
    "            print(\"No resources found.\")\n",
    "            return\n",
    "\n",
    "        # Check the matching resources\n",
    "        down_resources = self._matching_resources(pattern = resource_name)\n",
    "        if not down_resources:\n",
    "            print(f\"No resources found matching '{resource_name}'.\")\n",
    "            return\n",
    "\n",
    "        for file, md5 in down_resources:\n",
    "            errors = []\n",
    "            for mirror in self.mirrors:\n",
    "                url = os.path.join(mirror, file)\n",
    "                try:\n",
    "                    if not os.path.isfile(os.path.join(self.raw_folder, file)):\n",
    "                        print(f\"Downloading {file} from {mirror}\")\n",
    "                        download_url(url=url, root=self.raw_folder, filename=file, md5=md5)\n",
    "\n",
    "                except URLError as e:\n",
    "                    errors.append(e)\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                s = f\"Error downloading {file}:\\n\"\n",
    "                for mirror, err in zip(self.mirrors, errors):\n",
    "                    s += f\"Tried {mirror}, got:\\n{str(err)}\\n\"\n",
    "                raise RuntimeError(s)\n",
    "            \n",
    "    def _prepare_download_and_unpack(self, \n",
    "                          dataname: str, # Sting \n",
    "                          unpack: bool = True\n",
    "                          ) -> str:\n",
    "        \"\"\"\n",
    "        Common workflow: match resource, download if needed, unpack, and load.\n",
    "        Child classes should call this and implement their own load_data.\n",
    "        \"\"\"\n",
    "        matched_res = self._matching_resources(dataname)\n",
    "        if not matched_res:\n",
    "            raise ValueError(f\"No resources found matching '{dataname}'.\")\n",
    "\n",
    "        print(\"Matched resources to download:\")\n",
    "        for res, _ in matched_res:\n",
    "            print(f\"- {res}\")\n",
    "\n",
    "        # Download the resource if it does not exist in the raw folder \n",
    "        self._download_resource(resource_name=dataname)\n",
    "\n",
    "        # Unpack the resource if needed\n",
    "        if unpack:\n",
    "            self.data_folder = self._unpack_resource() # Unpacked folder with the data\n",
    "            return self.data_folder\n",
    "        else:\n",
    "            self.data_path = self._resource_datapath # If it does not need unpacking, just return the path to the resource file\n",
    "            return self._resource_datapath\n",
    "\n",
    "\n",
    "    def _unpack_resource(self) -> str:\n",
    "        \"\"\" Unpack the resource if it is compressed. \"\"\"\n",
    "\n",
    "        # path del resource sin unpack\n",
    "        assert os.path.exists(self._resource_datapath), f\"Resource {self._resource_datapath} does not exist. Please download it first.\" \n",
    "\n",
    "        # Check if the unpacked folder is already there\n",
    "        unpacked_folder = os.path.splitext(self._resource_datapath)[0]\n",
    "\n",
    "        if os.path.exists(unpacked_folder):\n",
    "            print(f\"Unpacked folder {unpacked_folder} already exists. Skipping unpacking.\")\n",
    "            return unpacked_folder\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                extract_archive(from_path=self._resource_datapath)\n",
    "                print(f\"Unpacked {self._resource_datapath} to {unpacked_folder}\")\n",
    "                return unpacked_folder\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error unpacking {self._resource_datapath}: {e}\")\n",
    "                return unpacked_folder  # Return the folder even if there was an error\n",
    "                \n",
    "                \n",
    "    @classmethod\n",
    "    def print_resources(cls):\n",
    "        print(f\"Resources for class {cls.__name__}:\")\n",
    "        for name, md5 in cls.resources:\n",
    "            print(f\"- {name} \")\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Database: {self.__class__.__name__}\\n\"\n",
    "            f\"Download: {[resname for resname, _ in self._resources_to_download] }\\n\"\n",
    "            f\"Load room: {self._resource_to_load}\\n\"\n",
    "            f\"Path to raw resource: {self._resource_datapath}\\n\"\n",
    "            f\"Path to unpacked data folder: {self.data_folder}\\n\"\n",
    "            f\"Sampling frequency: {self.fs} Hz\\n\"\n",
    "            f\"Number of microphones: {self.n_mics}\\n\"\n",
    "            f\"Number of total time samples: {self.nt}\\n\"\n",
    "            f\"Number of time samples selected: {self.signal_size}\\n\"\n",
    "            f\"Number of sources: {self.n_sources}\\n\"\n",
    "            f\"Signal start: {self.signal_start}\\n\"\n",
    "            f\"Signal size: {self.signal_size}\\n\"\n",
    "            f\"Source ID: {self.source_id}\"\n",
    "        )\n",
    "    \n",
    "    def _check_bounds_in_sample_size(self, number_of_time_samples: int) -> None:\n",
    "        \"\"\" Check if the start and size are within the bounds of the signal size. \"\"\"\n",
    "\n",
    "        T = number_of_time_samples\n",
    "        assert self._signal_start is not None\n",
    "        start_sample = self._signal_start\n",
    "        if self._signal_size is None:\n",
    "            self._signal_size = T - start_sample\n",
    "\n",
    "        assert self._signal_size is not None\n",
    "        last_sample = self._signal_start + self._signal_size\n",
    "\n",
    "        assert (start_sample >= 0 and start_sample < T), f\"The start_signal should be in [0, {T-1}].\"\n",
    "        assert (last_sample > 0 and last_sample <= T), f\"The size_signal should be in [1, {T-start_sample}].\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1534bc",
   "metadata": {},
   "source": [
    "### Zea database\n",
    "> Database from [Elias Zea](https://www.sciencedirect.com/science/article/abs/pii/S0022460X19304316) . It will inherit from DB_microphones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68c393",
   "metadata": {},
   "source": [
    "This is one of the RIR databases. It will have to implement it's own attributes:  \n",
    "    + `mirrors`  \n",
    "    + `resources`  \n",
    "    + `microphone spacing`  \n",
    "\n",
    "And the methods:  \n",
    "    + To check what resource to load  \n",
    "    + To download the resources  \n",
    "    + To unpack the downloaded resources  \n",
    "    + To load the selected resource (database/dataname)  \n",
    "    + To get the different attributes in the database: `dx`, `dt`, `fs`, `num_mics`, `num_sources`  \n",
    "    + And also the data related with the microphone recordings: `imic`, `position`, `time_samples`, `signal`  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ZeaRIR(DB_microphones):\n",
    "    \"\"\" ZeaRIR database. \"\"\"\n",
    "\n",
    "    mirrors = [\n",
    "            \"https://raw.githubusercontent.com/eliaszea/RIRIS/main/dependencies/measurementData/\"\n",
    "        ]\n",
    "\n",
    "    resources = [\n",
    "            (\"BalderRIR.mat\", \"bc904010041dc18e54a1a61b23ee3f99\"),\n",
    "            (\"FrejaRIR.mat\", \"1dedf2ab190ad48fbfa9403409418a1d\"),\n",
    "            (\"MuninRIR.mat\", \"5c90de0cbbc61128de332fffc64261c9\"),\n",
    "        ]\n",
    "    \n",
    "    _dx = 3e-2  # Distance between microphones in meters, as per the database documentation.\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str = \"./data\", # Path to the root directory of the database, where the data will be dowloaded\n",
    "                 dataname: str = \"Balder\", # String matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded). \n",
    "                 signal_start: int = 0, # Start index of the signal to load.\n",
    "                 signal_size: Optional[int] = None, # # int or None. Size of the signal to be extracted from the data, if None, the whole signal will be loaded.\n",
    "                 ):\n",
    "        super().__init__(root, dataname, signal_start, signal_size)\n",
    "\n",
    "        # Prepare the download and unpack the resource\n",
    "        filepath = self._prepare_download_and_unpack(dataname, unpack=False)\n",
    "        \n",
    "        self.data_folder = self.raw_folder\n",
    "\n",
    "        # The resource *.mat is not unpacked, so we can load it directly.\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        self.load_data(filepath)\n",
    "\n",
    "\n",
    "    def load_data(self, filepath: str):\n",
    "        \"\"\" Loads all the Matlab data from the given filepath.\"\"\"\n",
    "        print(f\"Loading the resource {filepath} ...\")\n",
    "        _rawdata = loadmat(filepath, simplify_cells=True)\n",
    "        self._fs = _rawdata['out']['fs']\n",
    "\n",
    "        T = _rawdata['out']['T']\n",
    "        M = _rawdata['out']['M']\n",
    "\n",
    "        # Check if the signal_start and signal_size are within the bounds of the signal size\n",
    "        self._check_bounds_in_sample_size(number_of_time_samples=T)\n",
    "        assert isinstance(self._signal_start, int) and isinstance(self._signal_size, int)\n",
    "        start_sample = self._signal_start\n",
    "        last_sample = self._signal_start + self._signal_size\n",
    "\n",
    "        self._RIR = _rawdata['out']['image'][start_sample:last_sample, :]  # Transpose to have (n_mics, n_sources, nt)\n",
    "\n",
    "        self._nmics = M\n",
    "        self._nt = T\n",
    "        self._n_sources = 1\n",
    "        self._source_id = 0\n",
    "\n",
    "    def get_mic(self, imic: int, start: Optional[int]=None, size: Optional[int]=None) -> np.ndarray:\n",
    "        \"\"\" Returns the signal of the microphone imic, starting at index start and with size size. \"\"\"\n",
    "        start = start if start is not None else self.signal_start\n",
    "        size = size if size is not None else self.signal_size\n",
    "        assert isinstance(start, int)\n",
    "        assert isinstance(size, int)\n",
    "        return self._RIR[start:start + size, imic]\n",
    "    \n",
    "    def get_pos(self, imic: int) -> np.ndarray:\n",
    "        \"\"\" Returns the position of the microphone imic in meters (x, y, z) \"\"\"\n",
    "        assert 0 <= imic < self.n_mics, f\"Microphone index {imic} out of range [0, {self.n_mics - 1}]\"\n",
    "        return  np.array([imic * self._dx, 0, 0])\n",
    "\n",
    "    def _unpack_resource(self):\n",
    "        \"\"\" .mat files does not need to be uncompressed. To avoid confusion, I return the path to the resource file directly. \"\"\"\n",
    "        return self._resource_datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ba658",
   "metadata": {},
   "source": [
    "#### Checks that Zea database works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched resources to download:\n",
      "- BalderRIR.mat\n",
      "- FrejaRIR.mat\n",
      "- MuninRIR.mat\n",
      "Loading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n"
     ]
    }
   ],
   "source": [
    "db = ZeaRIR(root=\"./data\", dataname=\"RIR\", signal_start=0, signal_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd2914",
   "metadata": {},
   "source": [
    "It has checked what resources match with dataname \"RIR\", and found three resources. It downloads all the matching resources. It only loads the data for the first resource \"Balder\", because there each object of this class should only return signals from the same room. To load other rooms I give a singular dataname corresponding to the name of that resource.  \n",
    "If the resources are already in the folder, it will skip the download:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ddca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db._download_resource(resource_name=\"Balder\") # Just return (no error message) because \"BalderRIR.mat\" is in the raw folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449321b",
   "metadata": {},
   "source": [
    "And we can check that the correct room and its parameters are properly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: ZeaRIR\n",
      "Download: ['BalderRIR.mat', 'FrejaRIR.mat', 'MuninRIR.mat']\n",
      "Load room: BalderRIR.mat\n",
      "Path to raw resource: ./data/ZeaRIR/raw/BalderRIR.mat\n",
      "Path to unpacked data folder: ./data/ZeaRIR/raw\n",
      "Sampling frequency: 11250 Hz\n",
      "Number of microphones: 100\n",
      "Number of total time samples: 3623\n",
      "Number of time samples selected: 128\n",
      "Number of sources: 1\n",
      "Signal start: 0\n",
      "Signal size: 128\n",
      "Source ID: 0\n"
     ]
    }
   ],
   "source": [
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789f4b1",
   "metadata": {},
   "source": [
    "We can check the data that it has loaded from the memory and that the main get methods work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7211e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk of data of size (128, 100)\n",
      "Output of get_mic  (4 time samples): [ 0.00041836  0.0001148  -0.00129174  0.00162724]\n",
      "Output of get_time (4 time samples): [0.00000000e+00 8.88888889e-05 1.77777778e-04 2.66666667e-04]\n",
      "Test of get_pos: [0.03 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded chunk of data of size {db._RIR.shape}\")\n",
    "print(f\"Output of get_mic  (4 time samples): {db.get_mic(imic=0, start=0, size=4)}\")\n",
    "print(f\"Output of get_time (4 time samples): {db.get_time(start=0, size=4)}\")\n",
    "print(f\"Test of get_pos: {db.get_pos(imic=1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de5721",
   "metadata": {},
   "source": [
    "Before implementing the downloading method, I used this code to test how to download the resources and what MD5 should I write for each resource (since it is not provided in the given mirror)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f82a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import calculate_md5, check_md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956f60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: BalderRIR.mat, MD5: bc904010041dc18e54a1a61b23ee3f99\n",
      "File: FrejaRIR.mat, MD5: 1dedf2ab190ad48fbfa9403409418a1d\n",
      "File: MuninRIR.mat, MD5: 5c90de0cbbc61128de332fffc64261c9\n"
     ]
    }
   ],
   "source": [
    "# db = ZeaRIR(root=\"./data\")\n",
    "for file, md5_class in db.resources:\n",
    "    url = os.path.join(db.mirrors[0], file)\n",
    "    download_url(url, root=db.raw_folder, filename=file)\n",
    "    md5 = calculate_md5(os.path.join(db.raw_folder, file))\n",
    "    print(f\"File: {file}, MD5: {md5}\")\n",
    "    assert check_md5(os.path.join(db.raw_folder, file), md5_class), (\n",
    "    f\"Check the MD5 of the resource '{file}' for the class '{db.__class__.__name__}' \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08caa8",
   "metadata": {},
   "source": [
    "It may be useful to check the name of the resources before instantiating an object (which will initiate the downloading process).  \n",
    "I can implement a class method to print the resources that can be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e86c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources for class ZeaRIR:\n",
      "- BalderRIR.mat \n",
      "- FrejaRIR.mat \n",
      "- MuninRIR.mat \n"
     ]
    }
   ],
   "source": [
    "ZeaRIR.print_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34c3f0",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "I am developing using nbdev, which includes an option `patch` from the library `fastcore`, that allows to implement a method of a class outside of the class definition, by declaring to which class it has to \"patch\" the method.  \n",
    "In the autogenerated .py file it will appear in a way that I am not that familiar, so I opted to just use patch for didactic purposes, but the exported code is already in the class definition.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(cls_method=True)  \n",
    "def print_resources(cls: DB_microphones):\n",
    "    print(f\"!!Method overwritten by a patch!!\")\n",
    "    print(f\"Resources for class {cls.__name__}:\")\n",
    "    for name, md5 in cls.resources:\n",
    "        print(f\"- {name} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848854c5",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-note}\n",
    "Pylance linting does not like `patch` and will underline it as a possible error.  \n",
    "I have added it directly to the class (the following code is just for testing purposes).\n",
    "([This is a callout from Quarto](https://quarto.org/docs/authoring/callouts.html#callout-types))\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537fd214",
   "metadata": {},
   "source": [
    "I can overwrite the method with patch (note the extra line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe16a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!Method overwritten by a patch!!\n",
      "Resources for class ZeaRIR:\n",
      "- BalderRIR.mat \n",
      "- FrejaRIR.mat \n",
      "- MuninRIR.mat \n"
     ]
    }
   ],
   "source": [
    "ZeaRIR.print_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f92021",
   "metadata": {},
   "source": [
    "### MeshRIR database\n",
    "> Database from [Shoichi Koyama](https://arxiv.org/abs/2106.10801), National Institute of Informatics, Tokyo, Japan . It will inherit from DB_micorphones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MeshRIR(DB_microphones):\n",
    "\n",
    "    mirrors = [\n",
    "        \"https://zenodo.org/records/10852693/files/\"\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"S1-M3969_npy.zip\", \"2cb598eb44bb9905560c545db7af3432\" ),\n",
    "        (\"S32-M441_npy.zip\", \"9818fc66b36513590e7abd071243d8e9\"), \n",
    "    ]\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str = \"./data\", # Path to the root directory of the database, where the data will be dowloaded\n",
    "                 dataname: str = \"S1\", # String matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded). \n",
    "                 signal_start: int = 0, # Start index of the signal to load.\n",
    "                 signal_size: Optional[int] = None, # Size of the signal to load. If None, the whole signal will be loaded.\n",
    "                 source_id: int = 0,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__(root, dataname, signal_start, signal_size)\n",
    "\n",
    "        # Prepare the download and unpack the resource\n",
    "        # This database unpacks files in a folder with the same name as the resource without the .zip extension \n",
    "        self.data_folder = self._prepare_download_and_unpack(dataname, unpack=True)\n",
    "        assert isinstance(self.data_folder, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "\n",
    "        # Load the data from the unpacked folder (also perform checks)\n",
    "        self._load_database_info()  # Load the database information from the data.json file\n",
    "        assert (source_id >= 0) and (source_id < self.n_sources) , f\"Database has {self.n_sources} sources. Choose source_id in [0, {self.n_sources-1}]. \"\n",
    "        self._source_id = source_id\n",
    "\n",
    "        # Loads src and mic positions, NOTE: maybe also load signals? load_all_data \n",
    "        # self.load_data(filepath=self.data_folder)       \n",
    "        self.load_src_and_mics_positions()\n",
    "\n",
    "\n",
    "    def load_data(self, filepath: str):\n",
    "        filepath=self.data_folder\n",
    "        self.load_src_and_mics_positions()\n",
    "\n",
    "    def _load_database_info(self):\n",
    "        \"\"\" Load the database information from the data.json file. \"\"\"\n",
    "        json_file = os.path.join(self.data_folder, \"data.json\")\n",
    "        with open(json_file, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        self._fs = json_data['samplerate']\n",
    "        T = json_data['ir length']\n",
    "        self._n_sources = json_data['number of sources']\n",
    "        self._nmics = json_data['number of points']\n",
    "\n",
    "        # Check that the folder contains all the signals in the database\n",
    "        nfiles = len([f for f in os.listdir(self.data_folder) if f.startswith('ir_') and f.endswith('.npy')])\n",
    "        assert self._nmics == nfiles, f\"ir_xxx.npy files = {nfiles}, should be {self._nmics}\"\n",
    "\n",
    "        # Check if the signal_start and signal_size are within the bounds of the signal size\n",
    "        self._check_bounds_in_sample_size(number_of_time_samples=T)\n",
    "        assert isinstance(self._signal_start, int) and isinstance(self._signal_size, int)\n",
    "        self._start_sample = self._signal_start\n",
    "        self._last_sample = self._signal_start + self._signal_size\n",
    "        self._nt = T\n",
    "\n",
    "    def load_src_and_mics_positions(self):\n",
    "        self.load_src_positions()  # Load source positions from the file\n",
    "        self.load_mic_positions()  # Load microphone positions from the file\n",
    "\n",
    "    def load_src_positions(self):\n",
    "        # Source position in the dataset\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        pos_src_path = os.path.join(filepath, 'pos_src.npy')\n",
    "        self._source_positions = np.load(pos_src_path)\n",
    "\n",
    "    def load_mic_positions(self):\n",
    "        # Position of the microphones\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        pos_mic_path = os.path.join(filepath, 'pos_mic.npy')\n",
    "        self._pos_mics = np.load(pos_mic_path) # (nmics, 3)  each row is (x,y,z) for a mic\n",
    "\n",
    "\n",
    "    def load_all_data(self):\n",
    "        # Concatenate vectors (source, signal) -> into -> (source, imic, signal)\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        data = np.concatenate( \n",
    "            [np.load(os.path.join(filepath, f'ir_{i}.npy'))[:,None,:]  \n",
    "             for i in range(self.n_mics)], # for all mics\n",
    "             axis = 1 ) # in axis 1 (mics)  (source, mics, signal)\n",
    "        return data\n",
    "\n",
    "    def load_mic(self, imic):\n",
    "        filepath = self.data_folder\n",
    "        assert isinstance(filepath, str), f\"Check if your resource has to be unpacked or not.\"\n",
    "        mic_signal = np.load(os.path.join(filepath, f'ir_{imic}.npy')) # (source, signal)\n",
    "        return mic_signal[self.source_id, :]\n",
    "        \n",
    "    def get_pos(self, imic: int)-> np.ndarray:\n",
    "        \"\"\" Returns the position of the microphone imic in meters (x, y, z) \"\"\"\n",
    "        if not hasattr(self, \"_pos_mics\"):\n",
    "            self.load_mic_positions()\n",
    "        assert 0 <= imic < self.n_mics, f\"Microphone index {imic} out of range [0, {self.n_mics - 1}]\"\n",
    "        return self._pos_mics[imic,:]\n",
    "     \n",
    "    def get_src_pos(self):\n",
    "        if not hasattr(self, \"_source_positions\"):\n",
    "            self.load_src_positions()\n",
    "        return self._source_positions[self.source_id]\n",
    "    \n",
    "    def get_mic(self, imic: int, start=None, size=None) -> np.ndarray:\n",
    "        \"\"\" Returns the signal of the microphone imic, starting at index start and with size size. \"\"\"\n",
    "        start = start if start is not None else self.signal_start\n",
    "        size = size if size is not None else self.signal_size\n",
    "        assert isinstance(start, int)\n",
    "        assert isinstance(size, int)\n",
    "        return self.load_mic(imic=imic)[start:start + size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d490714",
   "metadata": {},
   "source": [
    "Now let's check the MeshRIR database implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched resources to download:\n",
      "- S32-M441_npy.zip\n",
      "Unpacked folder ./data/MeshRIR/raw/S32-M441_npy already exists. Skipping unpacking.\n"
     ]
    }
   ],
   "source": [
    "db2 = MeshRIR(root=\"./data\", dataname=\"S32\", signal_start=0, signal_size=128, source_id=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c2fdb",
   "metadata": {},
   "source": [
    "Since this is a heavier database, I have already checked that the downloading method works.  \n",
    "This database requires to unzip the resource, the class has checked that the unpacked folder already exists, so it does not download and unpack the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d8768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database: MeshRIR\n",
      "Download: ['S32-M441_npy.zip']\n",
      "Load room: S32-M441_npy.zip\n",
      "Path to raw resource: ./data/MeshRIR/raw/S32-M441_npy.zip\n",
      "Path to unpacked data folder: ./data/MeshRIR/raw/S32-M441_npy\n",
      "Sampling frequency: 48000 Hz\n",
      "Number of microphones: 441\n",
      "Number of total time samples: 32768\n",
      "Number of time samples selected: 128\n",
      "Number of sources: 32\n",
      "Signal start: 0\n",
      "Signal size: 128\n",
      "Source ID: 31\n"
     ]
    }
   ],
   "source": [
    "print(db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb3eaf",
   "metadata": {},
   "source": [
    "Test of main get methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this database we do not preload all the database.\n",
      "Output of get_mic  (4 time samples): [0.00599654 0.00572385 0.00485317 0.00515282]\n",
      "Output of get_time (4 time samples): [0.00000000e+00 2.08333333e-05 4.16666667e-05 6.25000000e-05]\n",
      "Test of get_pos: [-0.4 -0.5  0. ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"In this database we do not preload all the database.\")\n",
    "print(f\"Output of get_mic  (4 time samples): {db2.get_mic(imic=0, start=0, size=4)}\")\n",
    "print(f\"Output of get_time (4 time samples): {db2.get_time(start=0, size=4)}\")\n",
    "print(f\"Test of get_pos: {db2.get_pos(imic=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2408961",
   "metadata": {},
   "source": [
    "## Good Practices (after coding)\n",
    "> Things that I have learnt, or thought they are interesting after coding this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0d54c",
   "metadata": {},
   "source": [
    "1. Use of **Inheritance**  \n",
    "    - There are different experimental databases but it is useful to crete a base class with the methods that I want to use for my applications.\n",
    "    - In the **base class** I try to define common attributes. The \"protected\" attributes starting with underscore ex: _fs. The \"private\" attributes starting with double-underscore ex: __fs. \n",
    "    - The class can have methods **getter** to return the protected and private attributes. In particular Iwill use ``@property`` to define which attributes I want to access. I can access ``obj._fs`` with ``obj.fs`` property method.\n",
    "    - It is possible to instantiate objecs of the **base class**, although it will not have the information we require, since this is an ``abstract`` class. To avoid wrong uses, there is the package abc (abstract base class) that includes definitions that are useful to define the behaviour of classes like this. \n",
    "    - Inheriting from ABC (Abstract Base Class) and declaring ``@abstractmethods`` that each subclass have to implement, avoids the instantiation of objects of any abstractclass or its subclasses is the abstractmethods are not overridden.\n",
    "    - This is useful to remind you that you have to implement all the abstractmethods before you can use a class.\n",
    "    - The base class contains the commonalities between databases so I do not have to repeat code.\n",
    "    - In the base class I can **__init__** only the strictly necessary attributes, but if there is a set of operations that may be used in different subclasses, I can define a method, like ``_prepare_data(self)``, and in the subclasses.__init__() I can use that method defined in the base class. **This avoids a case where a new subclass has a different init logic and I have to review the init logic of the base class**.  \n",
    "\n",
    "2. Logic and options to **download** the databases\n",
    "    - Inspired by **MNIST** I can write in the subclasses the class attributes ``mirrors`` and ``resources``, with the urls where I can download the files(resources).\n",
    "    - From **MNIST** I also use some downloading logic and what libraries to use to download, unpack and check data. \n",
    "    - When downloading files from github, do not use the url that can be seen in the explorer, but use the url where github saves the raw data: ``\"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_DATA_FOLDER}/\"`` substituting the ``USER``, ``REPO``, ``BRANCH``, ``PATH_DATA_FOLDER`` of the file that you want to download, as seen in the normal github url of the data.  \n",
    "\n",
    "3. Use a lot **``assert``**\n",
    "    - It is very useful to check for errors and that your parameters are supposed to be of a certain kind or in certain bounds.\n",
    "    - Sometimes Pylance or other linters show errors although the code is perfectly functional, because it can not detect the type of your data, an assert before the line of code where Pylance shows an error can tell Pylance that your data is gonna be of the type that is supposed to be, therefore, the operations such as + are compatible with those variables.\n",
    "\n",
    "4. Use the method ``__str__()``, to print useful information of the object, like different attributes, statistics, etc. Then use it as ``print(obj)``.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
