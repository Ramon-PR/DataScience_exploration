[
  {
    "objectID": "datasets/rir_torch_datasets.html",
    "href": "datasets/rir_torch_datasets.html",
    "title": "Implementation of torch Datasets based on the RIR databases",
    "section": "",
    "text": "Define what information from my databases I will provide to the model\n\n\n\n\nThe Dataset fixes the microphones that are known. For example, I set 4 microphones at specific locations to meassure the acoustics of an environment.\nIn every iteration it returns a dictionary containing information from 1 microphone (position, signal and time samples)\n\n\nclass DSRirFixedEnv(Dataset):\n    \"\"\" \n    Dataset with a fixed environment\n    In this version I let the targeted microphone (to be predicted) to be any of the micros in the data\n    (including those labeled as environment)\n    \"\"\"\n    def __init__(self, \n                 mic_database: DB_microphones, \n                 ids_env: List[int],\n                 ):\n        super().__init__()\n        self.db = mic_database\n        self.ids_env = ids_env        \n\n        # Environment microphones\n        self.env = {}\n        self.env['signal'] = [self.db.get_mic(i) for i in ids_env]\n        self.env['time'] = [self.db.get_time(i) for i in ids_env]\n        self.env['position'] = [self.db.get_pos(i) for i in ids_env]\n        # Change to torch tensors\n        self.env['signal'] = torch.from_numpy(np.stack(self.env['signal']).astype(np.float32))\n        self.env['time'] = torch.from_numpy(np.stack(self.env['time']).astype(np.float32))\n        self.env['position'] = torch.from_numpy(np.stack(self.env['position']).astype(np.float32))\n\n    def __len__(self):\n        return self.db.n_mics\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        In this version the environment is fixed, so in the __getitem__ \n        we only return the target \n        \"\"\"       \n        \n        return dict(signal=self.db.get_mic(idx),\n                    time=self.db.get_time(idx), \n                    position=self.db.get_pos(idx))\n \n    def get_env(self):\n        \"\"\"\n        Return the environment\n        \"\"\"\n        return self.env\n    \n    def __str__(self):\n\n        return ( \n            f\"Pytorch Dataset: {self.__class__.__name__}\\n\"\n            f\"With length: {self.__len__()} \\n\"\n            f\"Environment (mics ids): {self.ids_env}\"\n            f\"\\n\"+\n            self.db.__str__()\n        )\n\n\nds_Zea = DSRirFixedEnv(mic_database=ZeaRIR(\"./data\", dataname=\"Balder\", signal_start=0, signal_size=128),\n                      ids_env=[10, 30, 50])\nprint()\nprint(ds_Zea)\n\nMatched resources to download:\n- BalderRIR.mat\nLoading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n\nPytorch Dataset: DSRirFixedEnv\nWith length: 100 \nEnvironment (mics ids): [10, 30, 50]\nDatabase: ZeaRIR\nDownload: ['BalderRIR.mat']\nLoad room: BalderRIR.mat\nPath to raw resource: ./data/ZeaRIR/raw/BalderRIR.mat\nPath to unpacked data folder: ./data/ZeaRIR/raw\nSampling frequency: 11250 Hz\nNumber of microphones: 100\nNumber of total time samples: 3623\nNumber of time samples selected: 128\nNumber of sources: 1\nSignal start: 0\nSignal size: 128\nSource ID: 0\n\n\n\n# Accesing an element\nprint(f\"Length of dataset: {len(ds_Zea)}\")\nprint(\"Position of Target (index 1). \")\nprint(f\"using list indexing:   {ds_Zea[1]['position']} \") \nprint(f\"and using __getitem__: {ds_Zea.__getitem__(1)['position']} \")\n\n# Print the environment\nprint()\nprint(\"Environment \\nPositions:\")\nprint(ds_Zea.get_env()['position'])\n\nLength of dataset: 100\nPosition of Target (index 1). \nusing list indexing:   [0.03 0.   0.  ] \nand using __getitem__: [0.03 0.   0.  ] \n\nEnvironment \nPositions:\ntensor([[0.3000, 0.0000, 0.0000],\n        [0.9000, 0.0000, 0.0000],\n        [1.5000, 0.0000, 0.0000]])\n\n\n\n\n\n\nsource\n\n\n\n\n DS_random_pick (mic_database:DataScience_exploration.datasets.mics_databa\n                 ses.DB_microphones, n_ref_mics:int=4,\n                 max_combinations:int=1000)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmic_database\nDB_microphones\n\n\n\n\nn_ref_mics\nint\n4\nnumber of mics I will pick as my environment to interpolate\n\n\nmax_combinations\nint\n1000\nnumber of maximum combinations\n\n\n\n\nds_Mesh = DS_random_pick(mic_database=MeshRIR(root=\"./data\", dataname=\"S1\", signal_start=0, signal_size=128, source_id=0),\n                         n_ref_mics=4,\n                         max_combinations=20)\n\n\nenv, target = ds_Mesh[1]\nenv_p, target_p = ds_Mesh.__getitem__(1)\n\nprint()\n# Accesing an element\nprint(f\"Length of dataset: {len(ds_Mesh)}\")\nprint(\"Position of Target (index 1). \")\nprint(f\"using list indexing:   {target['position']} \") \nprint(f\"and using __getitem__: {target_p['position']} \")\n\n# Print the environment\nprint()\nprint(\"Environment \\nPositions:\")\nprint(env['position'])\n\nMatched resources to download:\n- S1-M3969_npy.zip\nUnpacked folder ./data/MeshRIR/raw/S1-M3969_npy already exists. Skipping unpacking.\n\nLength of dataset: 20\nPosition of Target (index 1). \nusing list indexing:   tensor([0.1500, 0.3000, 0.2000]) \nand using __getitem__: tensor([0.5000, 0.1500, 0.0500]) \n\nEnvironment \nPositions:\ntensor([[-0.2000,  0.2500,  0.1500],\n        [ 0.4500,  0.1500,  0.0000],\n        [-0.0500, -0.4500,  0.2000],\n        [-0.1000,  0.4000, -0.1500]])",
    "crumbs": [
      "datasets",
      "Implementation of torch Datasets based on the RIR databases"
    ]
  },
  {
    "objectID": "datasets/rir_torch_datasets.html#pytorch-datasets",
    "href": "datasets/rir_torch_datasets.html#pytorch-datasets",
    "title": "Implementation of torch Datasets based on the RIR databases",
    "section": "",
    "text": "Define what information from my databases I will provide to the model\n\n\n\n\nThe Dataset fixes the microphones that are known. For example, I set 4 microphones at specific locations to meassure the acoustics of an environment.\nIn every iteration it returns a dictionary containing information from 1 microphone (position, signal and time samples)\n\n\nclass DSRirFixedEnv(Dataset):\n    \"\"\" \n    Dataset with a fixed environment\n    In this version I let the targeted microphone (to be predicted) to be any of the micros in the data\n    (including those labeled as environment)\n    \"\"\"\n    def __init__(self, \n                 mic_database: DB_microphones, \n                 ids_env: List[int],\n                 ):\n        super().__init__()\n        self.db = mic_database\n        self.ids_env = ids_env        \n\n        # Environment microphones\n        self.env = {}\n        self.env['signal'] = [self.db.get_mic(i) for i in ids_env]\n        self.env['time'] = [self.db.get_time(i) for i in ids_env]\n        self.env['position'] = [self.db.get_pos(i) for i in ids_env]\n        # Change to torch tensors\n        self.env['signal'] = torch.from_numpy(np.stack(self.env['signal']).astype(np.float32))\n        self.env['time'] = torch.from_numpy(np.stack(self.env['time']).astype(np.float32))\n        self.env['position'] = torch.from_numpy(np.stack(self.env['position']).astype(np.float32))\n\n    def __len__(self):\n        return self.db.n_mics\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        In this version the environment is fixed, so in the __getitem__ \n        we only return the target \n        \"\"\"       \n        \n        return dict(signal=self.db.get_mic(idx),\n                    time=self.db.get_time(idx), \n                    position=self.db.get_pos(idx))\n \n    def get_env(self):\n        \"\"\"\n        Return the environment\n        \"\"\"\n        return self.env\n    \n    def __str__(self):\n\n        return ( \n            f\"Pytorch Dataset: {self.__class__.__name__}\\n\"\n            f\"With length: {self.__len__()} \\n\"\n            f\"Environment (mics ids): {self.ids_env}\"\n            f\"\\n\"+\n            self.db.__str__()\n        )\n\n\nds_Zea = DSRirFixedEnv(mic_database=ZeaRIR(\"./data\", dataname=\"Balder\", signal_start=0, signal_size=128),\n                      ids_env=[10, 30, 50])\nprint()\nprint(ds_Zea)\n\nMatched resources to download:\n- BalderRIR.mat\nLoading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n\nPytorch Dataset: DSRirFixedEnv\nWith length: 100 \nEnvironment (mics ids): [10, 30, 50]\nDatabase: ZeaRIR\nDownload: ['BalderRIR.mat']\nLoad room: BalderRIR.mat\nPath to raw resource: ./data/ZeaRIR/raw/BalderRIR.mat\nPath to unpacked data folder: ./data/ZeaRIR/raw\nSampling frequency: 11250 Hz\nNumber of microphones: 100\nNumber of total time samples: 3623\nNumber of time samples selected: 128\nNumber of sources: 1\nSignal start: 0\nSignal size: 128\nSource ID: 0\n\n\n\n# Accesing an element\nprint(f\"Length of dataset: {len(ds_Zea)}\")\nprint(\"Position of Target (index 1). \")\nprint(f\"using list indexing:   {ds_Zea[1]['position']} \") \nprint(f\"and using __getitem__: {ds_Zea.__getitem__(1)['position']} \")\n\n# Print the environment\nprint()\nprint(\"Environment \\nPositions:\")\nprint(ds_Zea.get_env()['position'])\n\nLength of dataset: 100\nPosition of Target (index 1). \nusing list indexing:   [0.03 0.   0.  ] \nand using __getitem__: [0.03 0.   0.  ] \n\nEnvironment \nPositions:\ntensor([[0.3000, 0.0000, 0.0000],\n        [0.9000, 0.0000, 0.0000],\n        [1.5000, 0.0000, 0.0000]])\n\n\n\n\n\n\nsource\n\n\n\n\n DS_random_pick (mic_database:DataScience_exploration.datasets.mics_databa\n                 ses.DB_microphones, n_ref_mics:int=4,\n                 max_combinations:int=1000)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmic_database\nDB_microphones\n\n\n\n\nn_ref_mics\nint\n4\nnumber of mics I will pick as my environment to interpolate\n\n\nmax_combinations\nint\n1000\nnumber of maximum combinations\n\n\n\n\nds_Mesh = DS_random_pick(mic_database=MeshRIR(root=\"./data\", dataname=\"S1\", signal_start=0, signal_size=128, source_id=0),\n                         n_ref_mics=4,\n                         max_combinations=20)\n\n\nenv, target = ds_Mesh[1]\nenv_p, target_p = ds_Mesh.__getitem__(1)\n\nprint()\n# Accesing an element\nprint(f\"Length of dataset: {len(ds_Mesh)}\")\nprint(\"Position of Target (index 1). \")\nprint(f\"using list indexing:   {target['position']} \") \nprint(f\"and using __getitem__: {target_p['position']} \")\n\n# Print the environment\nprint()\nprint(\"Environment \\nPositions:\")\nprint(env['position'])\n\nMatched resources to download:\n- S1-M3969_npy.zip\nUnpacked folder ./data/MeshRIR/raw/S1-M3969_npy already exists. Skipping unpacking.\n\nLength of dataset: 20\nPosition of Target (index 1). \nusing list indexing:   tensor([0.1500, 0.3000, 0.2000]) \nand using __getitem__: tensor([0.5000, 0.1500, 0.0500]) \n\nEnvironment \nPositions:\ntensor([[-0.2000,  0.2500,  0.1500],\n        [ 0.4500,  0.1500,  0.0000],\n        [-0.0500, -0.4500,  0.2000],\n        [-0.1000,  0.4000, -0.1500]])",
    "crumbs": [
      "datasets",
      "Implementation of torch Datasets based on the RIR databases"
    ]
  },
  {
    "objectID": "datasets/rir_torch_datasets.html#pytorch-lightning-datamodules",
    "href": "datasets/rir_torch_datasets.html#pytorch-lightning-datamodules",
    "title": "Implementation of torch Datasets based on the RIR databases",
    "section": "Pytorch lightning Datamodules",
    "text": "Pytorch lightning Datamodules\n\nThe pytorch lightning Datamodule organizes the torch Datasets with the operations that will have to be performed during the stages “fit” and “test”. It also includes information about the Dataloader that will be used for the training.\n\n\ndef ensure_list(x):\n    if isinstance(x, Dataset):\n        return [x]\n    elif isinstance(x, list):\n        return x\n    elif x is None:\n        return []\n    else:\n        raise TypeError(f\"Expected Dataset or list of Datasets, got {type(x)}\")\n    \nclass DM_PL_DataModule(L.LightningDataModule):\n    def __init__(self, \n                 ls_datasets_train: List[torch.utils.data.Dataset] = [], \n                 ls_datasets_test: List[torch.utils.data.Dataset] = [],\n                 batch_size: int = 64, num_workers: int = 0, \n                 ):\n        super().__init__()\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.ls_datasets_train = ensure_list(ls_datasets_train) \n        self.ls_datasets_test = ensure_list(ls_datasets_train)\n\n    def setup(self, stage):\n        if stage == \"fit\":\n            self.ds_train, self.ds_val = random_split( ConcatDataset(self.ls_datasets_train), \n                                                        [0.8, 0.2])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\":\n            self.ds_test = ConcatDataset(self.ls_datasets_test)\n\n    def train_dataloader(self):\n        return DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True,\n            num_workers=self.num_workers, pin_memory=False, collate_fn=None)\n\n    def val_dataloader(self):\n        return DataLoader(self.ds_val, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def test_dataloader(self):\n        return DataLoader(self.ds_test, batch_size=self.batch_size)",
    "crumbs": [
      "datasets",
      "Implementation of torch Datasets based on the RIR databases"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataScience_exploration",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "DataScience_exploration"
    ]
  },
  {
    "objectID": "index.html#first-steps",
    "href": "index.html#first-steps",
    "title": "DataScience_exploration",
    "section": "First steps",
    "text": "First steps\n\nCheck that you will install everything in an environment that you want.\n\n$ conda create -n my_env\n$ conda activate my_env\n\nInstall nbdev (-U for upgrade, pip will upgrade it to the newest available version.)\n\n$ pip install -U nbdev\n\ngit clone the repo and install it in Development mode\n\n$ git clone https://github.com/Ramon-PR/DataScience_exploration.git\n$ pip install -e .\n\nIn settings.ini you can see the packages that are required in the section “requirements”. Do not forget to update this section if you install a package.",
    "crumbs": [
      "DataScience_exploration"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "DataScience_exploration",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall DataScience_exploration in Development mode\n# make sure DataScience_exploration package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to DataScience_exploration\n$ nbdev_prepare",
    "crumbs": [
      "DataScience_exploration"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "DataScience_exploration",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/Ramon-PR/DataScience_exploration.git\nor from conda\n$ conda install -c Ramon-PR DataScience_exploration\nor from pypi\n$ pip install DataScience_exploration\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "DataScience_exploration"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "DataScience_exploration",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "DataScience_exploration"
    ]
  },
  {
    "objectID": "datasets/rir_databases.html",
    "href": "datasets/rir_databases.html",
    "title": "Class for RIR measurement databases",
    "section": "",
    "text": "We will define many class properties with @property and to make sure all the attributes are initialized before their use, we define the following method\n\nsource\n\n\n\n checked_property (attr_name:str, attr_type:type=&lt;class 'object'&gt;,\n                   doc:Optional[str]=None)\n\nEnsures that the attribute is initialized before accessing it.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nattr_name\nstr\n\nstring with the name of the protected attribute to access, example: ’_fs’\n\n\nattr_type\ntype\nobject\nType of the attribute: for _fs for example is float\n\n\ndoc\nOptional\nNone\nString containing a descrption of the class attribute\n\n\n\nExample of use:\n\nclass Mics(ABC):  \n    _fs: Optional[int] = None  \n    fs = checked_property('_fs', float)\n\nAnd if we use the property to access _fs without it being initialized, it should give an error\n\nmic = Mics()\nprint(mic._fs)  # ``_fs`` is None,\ntry:\n    print(mic.fs)  # ❌ But the property ``fs`` requires _fs to be initialized to a float value\nexcept ValueError as e:\n    print(f\"Caught ValueError: {e}\")\n\nNone\nCaught ValueError: Attribute '_fs' is not initialized.",
    "crumbs": [
      "datasets",
      "Class for RIR measurement databases"
    ]
  },
  {
    "objectID": "datasets/rir_databases.html#a.-helper-funtions",
    "href": "datasets/rir_databases.html#a.-helper-funtions",
    "title": "Class for RIR measurement databases",
    "section": "",
    "text": "We will define many class properties with @property and to make sure all the attributes are initialized before their use, we define the following method\n\nsource\n\n\n\n checked_property (attr_name:str, attr_type:type=&lt;class 'object'&gt;,\n                   doc:Optional[str]=None)\n\nEnsures that the attribute is initialized before accessing it.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nattr_name\nstr\n\nstring with the name of the protected attribute to access, example: ’_fs’\n\n\nattr_type\ntype\nobject\nType of the attribute: for _fs for example is float\n\n\ndoc\nOptional\nNone\nString containing a descrption of the class attribute\n\n\n\nExample of use:\n\nclass Mics(ABC):  \n    _fs: Optional[int] = None  \n    fs = checked_property('_fs', float)\n\nAnd if we use the property to access _fs without it being initialized, it should give an error\n\nmic = Mics()\nprint(mic._fs)  # ``_fs`` is None,\ntry:\n    print(mic.fs)  # ❌ But the property ``fs`` requires _fs to be initialized to a float value\nexcept ValueError as e:\n    print(f\"Caught ValueError: {e}\")\n\nNone\nCaught ValueError: Attribute '_fs' is not initialized.",
    "crumbs": [
      "datasets",
      "Class for RIR measurement databases"
    ]
  },
  {
    "objectID": "datasets/rir_databases.html#b.-database-for-microphones",
    "href": "datasets/rir_databases.html#b.-database-for-microphones",
    "title": "Class for RIR measurement databases",
    "section": "B. Database for microphones",
    "text": "B. Database for microphones\n\nThe base class to handle RIR measurements.\n\nThis class defines common properties and methods for the different RIR databases that will inherit from it. The class DB_microphones will be an abstract class (from abc import ABC, abstractmethod)\n\nABC: base clase to declare an Abstract Base Class\n\nabstractmethod: it is a decorator to indicate which methods have to be implemented by the subclasses\n\nThis is useful since this base class can not be implemented and will force the subclasses to implement certain methods abstractmethod\nInspired by MNIST dataset, we will download the data in a folder structure like ./root/class_name/raw.\n\nroot: is a parameter passed to the class\nclass_name: is the name of the class used to download the database\n\nraw: is the subfolder where the raw data is downloaded\n\nand we will include a mirror list with the urls where we can find the data to download, and a list resources that contains tuples with the name of the file to download and it’s md5 checksum.\n\nBase class\n\nsource\n\n\nDB_microphones\n\n DB_microphones (root:str='./data', dataname:str='RIR',\n                 signal_start:int=0, signal_size:Optional[int]=None)\n\nBase class for microphone databases. Defines methods: get_mic, get_pos, get_time and class @property such as .fs, .nt, .n_mics, .n_sources, …\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\n./data\nPath to the root directory of the database, where the data will be dowloaded\n\n\ndataname\nstr\nRIR\nString matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded).\n\n\nsignal_start\nint\n0\nStart index of the signal in the data\n\n\nsignal_size\nOptional\nNone\nint or None. Size of the signal to be extracted from the data, if None, the whole signal will be loaded.\n\n\n\n\n\nZea database\n\nDatabase from Elias Zea . It will inherit from DB_microphones\n\nThis is one of the RIR databases. It will have to implement it’s own attributes:\n+ mirrors\n+ resources\n+ microphone spacing\nAnd the methods:\n+ To check what resource to load\n+ To download the resources\n+ To unpack the downloaded resources\n+ To load the selected resource (database/dataname)\n+ To get the different attributes in the database: dx, dt, fs, num_mics, num_sources\n+ And also the data related with the microphone recordings: imic, position, time_samples, signal\n\nsource\n\n\nZeaRIR\n\n ZeaRIR (root:str='./data', dataname:str='Balder', signal_start:int=0,\n         signal_size:Optional[int]=None)\n\nZeaRIR database.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\n./data\nPath to the root directory of the database, where the data will be dowloaded\n\n\ndataname\nstr\nBalder\nString matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded).\n\n\nsignal_start\nint\n0\nStart index of the signal to load.\n\n\nsignal_size\nOptional\nNone\n# int or None. Size of the signal to be extracted from the data, if None, the whole signal will be loaded.\n\n\n\n\nChecks that Zea database works\n\ndb = ZeaRIR(root=\"./data\", dataname=\"RIR\", signal_start=0, signal_size=128)\n\nMatched resources to download:\n- BalderRIR.mat\n- FrejaRIR.mat\n- MuninRIR.mat\nLoading the resource ./data/ZeaRIR/raw/BalderRIR.mat ...\n\n\nIt has checked what resources match with dataname “RIR”, and found three resources. It downloads all the matching resources. It only loads the data for the first resource “Balder”, because there each object of this class should only return signals from the same room. To load other rooms I give a singular dataname corresponding to the name of that resource.\nIf the resources are already in the folder, it will skip the download:\n\ndb._download_resource(resource_name=\"Balder\") # Just return (no error message) because \"BalderRIR.mat\" is in the raw folder\n\nAnd we can check that the correct room and its parameters are properly loaded\n\nprint(db)\n\nDatabase: ZeaRIR\nDownload: ['BalderRIR.mat', 'FrejaRIR.mat', 'MuninRIR.mat']\nLoad room: BalderRIR.mat\nPath to raw resource: ./data/ZeaRIR/raw/BalderRIR.mat\nPath to unpacked data folder: ./data/ZeaRIR/raw\nSampling frequency: 11250 Hz\nNumber of microphones: 100\nNumber of total time samples: 3623\nNumber of time samples selected: 128\nNumber of sources: 1\nSignal start: 0\nSignal size: 128\nSource ID: 0\n\n\nWe can check the data that it has loaded from the memory and that the main get methods work:\n\nprint(f\"Loaded chunk of data of size {db._RIR.shape}\")\nprint(f\"Output of get_mic  (4 time samples): {db.get_mic(imic=0, start=0, size=4)}\")\nprint(f\"Output of get_time (4 time samples): {db.get_time(start=0, size=4)}\")\nprint(f\"Test of get_pos: {db.get_pos(imic=1)}\")\n\nLoaded chunk of data of size (128, 100)\nOutput of get_mic  (4 time samples): [ 0.00041836  0.0001148  -0.00129174  0.00162724]\nOutput of get_time (4 time samples): [0.00000000e+00 8.88888889e-05 1.77777778e-04 2.66666667e-04]\nTest of get_pos: [0.03 0.   0.  ]\n\n\nBefore implementing the downloading method, I used this code to test how to download the resources and what MD5 should I write for each resource (since it is not provided in the given mirror).\n\nfrom torchvision.datasets.utils import calculate_md5, check_md5\n\n\n# db = ZeaRIR(root=\"./data\")\nfor file, md5_class in db.resources:\n    url = os.path.join(db.mirrors[0], file)\n    download_url(url, root=db.raw_folder, filename=file)\n    md5 = calculate_md5(os.path.join(db.raw_folder, file))\n    print(f\"File: {file}, MD5: {md5}\")\n    assert check_md5(os.path.join(db.raw_folder, file), md5_class), (\n    f\"Check the MD5 of the resource '{file}' for the class '{db.__class__.__name__}' \"\n)\n\nFile: BalderRIR.mat, MD5: bc904010041dc18e54a1a61b23ee3f99\nFile: FrejaRIR.mat, MD5: 1dedf2ab190ad48fbfa9403409418a1d\nFile: MuninRIR.mat, MD5: 5c90de0cbbc61128de332fffc64261c9\n\n\nIt may be useful to check the name of the resources before instantiating an object (which will initiate the downloading process).\nI can implement a class method to print the resources that can be downloaded.\n\nZeaRIR.print_resources()\n\nResources for class ZeaRIR:\n- BalderRIR.mat \n- FrejaRIR.mat \n- MuninRIR.mat \n\n\n\n\n\n\n\n\nNote\n\n\n\nI am developing using nbdev, which includes an option patch from the library fastcore, that allows to implement a method of a class outside of the class definition, by declaring to which class it has to “patch” the method.\nIn the autogenerated .py file it will appear in a way that I am not that familiar, so I opted to just use patch for didactic purposes, but the exported code is already in the class definition.\n\n\n\n@patch(cls_method=True)  \ndef print_resources(cls: DB_microphones):\n    print(f\"!!Method overwritten by a patch!!\")\n    print(f\"Resources for class {cls.__name__}:\")\n    for name, md5 in cls.resources:\n        print(f\"- {name} \")\n\n\n\n\n\n\n\nNote\n\n\n\nPylance linting does not like patch and will underline it as a possible error.\nI have added it directly to the class (the following code is just for testing purposes). (This is a callout from Quarto)\n\n\nI can overwrite the method with patch (note the extra line)\n\nZeaRIR.print_resources()\n\n!!Method overwritten by a patch!!\nResources for class ZeaRIR:\n- BalderRIR.mat \n- FrejaRIR.mat \n- MuninRIR.mat \n\n\n\n\n\nMeshRIR database\n\nDatabase from Shoichi Koyama, National Institute of Informatics, Tokyo, Japan . It will inherit from DB_micorphones\n\n\nsource\n\n\nMeshRIR\n\n MeshRIR (root:str='./data', dataname:str='S1', signal_start:int=0,\n          signal_size:Optional[int]=None, source_id:int=0)\n\nBase class for microphone databases. Defines methods: get_mic, get_pos, get_time and class @property such as .fs, .nt, .n_mics, .n_sources, …\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr\n./data\nPath to the root directory of the database, where the data will be dowloaded\n\n\ndataname\nstr\nS1\nString matching the name of the resources to download and load. (if several resources are available, all will be downloaded but only the first one will be loaded).\n\n\nsignal_start\nint\n0\nStart index of the signal to load.\n\n\nsignal_size\nOptional\nNone\nSize of the signal to load. If None, the whole signal will be loaded.\n\n\nsource_id\nint\n0\n\n\n\n\nNow let’s check the MeshRIR database implementation:\n\ndb2 = MeshRIR(root=\"./data\", dataname=\"S32\", signal_start=0, signal_size=128, source_id=31)\n\nMatched resources to download:\n- S32-M441_npy.zip\nUnpacked folder ./data/MeshRIR/raw/S32-M441_npy already exists. Skipping unpacking.\n\n\nSince this is a heavier database, I have already checked that the downloading method works.\nThis database requires to unzip the resource, the class has checked that the unpacked folder already exists, so it does not download and unpack the resource.\n\nprint(db2)\n\nDatabase: MeshRIR\nDownload: ['S32-M441_npy.zip']\nLoad room: S32-M441_npy.zip\nPath to raw resource: ./data/MeshRIR/raw/S32-M441_npy.zip\nPath to unpacked data folder: ./data/MeshRIR/raw/S32-M441_npy\nSampling frequency: 48000 Hz\nNumber of microphones: 441\nNumber of total time samples: 32768\nNumber of time samples selected: 128\nNumber of sources: 32\nSignal start: 0\nSignal size: 128\nSource ID: 31\n\n\nTest of main get methods:\n\nprint(f\"In this database we do not preload all the database.\")\nprint(f\"Output of get_mic  (4 time samples): {db2.get_mic(imic=0, start=0, size=4)}\")\nprint(f\"Output of get_time (4 time samples): {db2.get_time(start=0, size=4)}\")\nprint(f\"Test of get_pos: {db2.get_pos(imic=1)}\")\n\nIn this database we do not preload all the database.\nOutput of get_mic  (4 time samples): [0.00599654 0.00572385 0.00485317 0.00515282]\nOutput of get_time (4 time samples): [0.00000000e+00 2.08333333e-05 4.16666667e-05 6.25000000e-05]\nTest of get_pos: [-0.4 -0.5  0. ]",
    "crumbs": [
      "datasets",
      "Class for RIR measurement databases"
    ]
  },
  {
    "objectID": "datasets/rir_databases.html#good-practices-after-coding",
    "href": "datasets/rir_databases.html#good-practices-after-coding",
    "title": "Class for RIR measurement databases",
    "section": "Good Practices (after coding)",
    "text": "Good Practices (after coding)\n\nThings that I have learnt, or thought they are interesting after coding this notebook\n\n\nUse of Inheritance\n\nThere are different experimental databases but it is useful to crete a base class with the methods that I want to use for my applications.\nIn the base class I try to define common attributes. The “protected” attributes starting with underscore ex: _fs. The “private” attributes starting with double-underscore ex: __fs.\nThe class can have methods getter to return the protected and private attributes. In particular Iwill use @property to define which attributes I want to access. I can access obj._fs with obj.fs property method.\nIt is possible to instantiate objecs of the base class, although it will not have the information we require, since this is an abstract class. To avoid wrong uses, there is the package abc (abstract base class) that includes definitions that are useful to define the behaviour of classes like this.\nInheriting from ABC (Abstract Base Class) and declaring @abstractmethods that each subclass have to implement, avoids the instantiation of objects of any abstractclass or its subclasses is the abstractmethods are not overridden.\nThis is useful to remind you that you have to implement all the abstractmethods before you can use a class.\nThe base class contains the commonalities between databases so I do not have to repeat code.\nIn the base class I can init only the strictly necessary attributes, but if there is a set of operations that may be used in different subclasses, I can define a method, like _prepare_data(self), and in the subclasses.__init__() I can use that method defined in the base class. This avoids a case where a new subclass has a different init logic and I have to review the init logic of the base class.\n\nLogic and options to download the databases\n\nInspired by MNIST I can write in the subclasses the class attributes mirrors and resources, with the urls where I can download the files(resources).\nFrom MNIST I also use some downloading logic and what libraries to use to download, unpack and check data.\nWhen downloading files from github, do not use the url that can be seen in the explorer, but use the url where github saves the raw data: \"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_DATA_FOLDER}/\" substituting the USER, REPO, BRANCH, PATH_DATA_FOLDER of the file that you want to download, as seen in the normal github url of the data.\n\nUse a lot assert\n\nIt is very useful to check for errors and that your parameters are supposed to be of a certain kind or in certain bounds.\nSometimes Pylance or other linters show errors although the code is perfectly functional, because it can not detect the type of your data, an assert before the line of code where Pylance shows an error can tell Pylance that your data is gonna be of the type that is supposed to be, therefore, the operations such as + are compatible with those variables.\n\nUse the method __str__(), to print useful information of the object, like different attributes, statistics, etc. Then use it as print(obj).",
    "crumbs": [
      "datasets",
      "Class for RIR measurement databases"
    ]
  }
]